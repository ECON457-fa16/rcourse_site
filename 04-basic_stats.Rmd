---
---

# Basic statistics

Of course, R was written by statisticians, for statisticians.  We're not going 
to go deep into stats - partly because I'm not really that qualified to teach it, 
and because we don't have time to cover all of the potential needs that people 
in the course will have.  But we can cover a few of the basics, and introduce 
the common **R** way of fitting statistical models.

### t-test

We'll keep going with our Iris data; we want to test if petal lengths are 
significantly different between _Iris setosa_ and _Iris virginica_; so we can use a 
basic two-sample t-test.

First, let's search the help to find out what functions are avaible: `??"t-test"`
.  Student's t-test is the one we want.  There are a few variations of the 
t-test available.  If we are testing a single sample against a known value (for 
example, find out if something is different from 0), we would use the 
single-sample t-test like so:

```{r sst-test}
# Simulate some data with a normal distribution, a mean of 0, and sd of 1.
data <- rnorm(100)
mean(data)
t.test(data, mu=0)
## Unsurprisingly, not significant.
```

For the iris data, we want to use a two-sample t-test.  I like using the formula 
specification because it's similar to how many other statistical tests are 
specified: ` t.test(Value ~ factor, data=)`

Since we're only interested in _setosa_ and _virginica_, we need to get rid of 
_versicolor_.

```{r dropfort-test}
require(dplyr)
setosa.virginica <- filter(dat, Species != "versicolor")
summary(setosa.virginica)
setosa.virginica <- droplevels(setosa.virginica)
```

```{r tst-test}
t.test(Sepal.Length ~ Species, data=setosa.virginica)
```

### Simple linear regression

Let's see if petal length can be used to predict petal width for a single species; _Iris setosa_. (Note; this is a bit of a misuse of regression, as regression 
usually implies causation, but as an example it will suffice).

The following code fits the basic linear regression model where `Petal.Length` 
is the response variable and `Petal.Width` is the predictor variable, then 
prints a summary of the model.

```{r olsreg}
setosa.dat <- filter(dat, Species == "setosa")
petal.reg <- lm(Petal.Length ~ Petal.Width, data=setosa.dat)
summary(petal.reg)
```

#### Plot the data with the regression line, along with confidence limits

```{r olsreg-plot}
p <- ggplot(setosa.dat, aes(x = Petal.Width, y = Petal.Length)) + geom_point()

dummy <- data.frame(Petal.Width = seq(from = min(setosa.dat$Petal.Width), 
                                     to = max(setosa.dat$Petal.Width), 
                                     length.out = 100))
                   
pred <- predict(petal.reg, newdata=dummy, interval = "conf")

dummy <- cbind(dummy, pred)

p + geom_line(data = dummy, aes(y = fit)) + 
  geom_line(data = dummy, aes(y = lwr), linetype = 'dashed') + 
  geom_line(data = dummy, aes(y = upr), linetype = 'dashed')

```

ggplot2 will also generate a fitted line and confidence intervals for you - 
which is useful, but only works for a univariate relationship ... it's also nice 
to do it yourself as above so you *know* that the fit is coming directly from 
regression model you ran.

```{r olsreg-ggplot-lm}
p + geom_smooth(method="lm")
```


#### Checking Assumptions

We can check these assumptions of the model by plotting the residuals vs the 
fitted values.

```{r ggplot-resids}
fitted <- fitted(petal.reg)
residuals <- resid(petal.reg)

ggplot(data=NULL, aes(x = fitted, y = residuals)) + geom_point() + 
  geom_hline(yintercept = 0)
```

We can check also assumptions using `plot()`.  There are actually a bunch of 
different `plot` methods in R, which are dispatched depending on the type of 
object you call them on.  When you call plot on an `lm` object, a series of 
diagnostic plots is created to help us check the assumptions of the `lm` object.

```{r olsreg-check}
plot(petal.reg)
```

Get more information on these plots by checking `?plot.lm`.

### Analysis of Variance (ANOVA)

Now say we want to compare an attribute among all three species, then we can't 
use a t-test; we have to use an ANOVA.  Since an ANOVA is simply a linear 
regression model with a categorical rather than continuous predictor variable, we 
still use the `lm()` function.  Let's test for differences in petal length among 
all three species.

```{r anova}
petal_length.aov <- lm(Petal.Length ~ Species, data=dat)
summary(petal_length.aov)
anova(petal_length.aov)
```

#### Plot

```{r petal-box-plot}
ggplot(data = dat, aes(x = Species, y = Petal.Length)) + geom_boxplot()
ggplot(data = dat, aes(x = Species, y = Petal.Length)) + geom_point()
ggplot(data = dat, aes(x = Species, y = Petal.Length)) + geom_jitter()

```

#### Check assumptions

```{r ggplot-aov-resids}
fitted <- fitted(petal_length.aov)
residuals <- resid(petal_length.aov)

ggplot(data=NULL, aes(x = fitted, y = residuals)) + geom_point() + 
  geom_hline(yintercept = 0)
```

### Generalized linear models: Logistic regression

Say you want to know whether elevation can predict whether or not a particular 
species of beetle is present (all other things being equal of course). You walk 
up a hillside, starting at 100m elevation and sampling for the beetle every 10m 
until you reach 1000m.  At each stop you record whether or the beetle is present 
(`1`) or absent (`0`).

First, let's simulate some data

```{r glm-binom-sim}
## Generate a sequence of elevations
elev <- seq(100, 1000, by=10)

# Generate a vector of probabilities the same length as `elev` with increasing 
# probabilities
probs <- 0:length(elev) / length(elev)

## Generate a sequence of 0's and 1's
pres <- rbinom(length(elev), 1, prob=probs)

## combine into a data frame and remove consituent parts
elev_pres.data <- data.frame(elev, pres)
rm(elev, pres)

## Plot the data
ggplot(elev_pres.data, aes(x = elev, y = pres)) + geom_point()
```

Presence / absence data is a classic example of where to use logistic regression; 
the outcome is binary (0 or 1), and the predictor variable is continuous 
(elevation, in this case).  Logisitic regression is a particular type of model 
in the family of _Generalized Linear Models_.  Where ordinary least squares 
regression assumes a normal disribution of the response variable, _Generalized 
linear models_ assume a different distribution.  Logistic regression assumes a 
binomial distribution (outcome will be in one of two states).  Another common 
example is the poisson distribution, which is often useful for count data.  
Implementing GLMs is relatively straightforward using the `glm()` function.  You 
specify the model formula in the same way as in `lm()`, and specify the 
distribution you want in the _family_ parameter.

```{r glm-binom}
lr1 <- glm(pres ~ elev, data=elev_pres.data, family=binomial)
summary(lr1)
```

So let's add the curve generated by the logistic regression to the plot:

```{r lr-plot}
ggplot(elev_pres.data, aes(x = elev, y = pres)) + 
  geom_point() + 
  geom_line(aes(y = predict(lr1, type="response")))

```



### More advanced linear models and model selection using AIC

```{r}
mod1 <- lm(Sepal.Length ~ Sepal.Width * Species, data=iris)
mod2 <- lm(Sepal.Length ~ Sepal.Width + Species, data=iris) # ANCOVA
mod3 <- lm(Sepal.Length ~ Sepal.Width, data=iris)
mod4 <- lm(Sepal.Length ~ Species, data=iris)

AIC(mod1, mod2, mod3, mod4)
```

Let's plot the data:
```{r ggplot-adv-lm}
ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length, colour=Species, group=Species)) + 
  geom_point() + 
  geom_smooth(method="lm", formula = y ~ x)
```



